import numpy as np
from keras.datasets import mnist

# Grab data
(x_train, y_train), (x_test, y_test) = mnist.load_data()

# Reshape and scale data
x_train_vec = x_train.reshape((60000, 28, 28, 1)) / 255.0 
x_test_vec = x_test.reshape((10000, 28, 28, 1)) / 255.0

# One-hot encode labels
def to_one_hot(labels, dimension=10):
    results = np.zeros((len(labels), dimension))
    for i, label in enumerate(labels):
        results[i, label] = 1.
    return results
y_train_vec = to_one_hot(y_train)
y_test_vec = to_one_hot(y_test)

# Partition into training and validation sets
rand_indices = np.random.permutation(60000)
train_indices = rand_indices[0:50000]
valid_indices = rand_indices[50000:60000]
x_valid_vec = x_train_vec[valid_indices, :, :, :]
y_valid_vec = y_train_vec[valid_indices, :]
x_train_vec = x_train_vec[train_indices, :, :, :]
y_train_vec = y_train_vec[train_indices, :]

from keras import models
from keras import layers

# Build model
model = models.Sequential()
model.add(layers.Conv2D(10, (5, 5), activation='relu', input_shape=(28, 28, 1)))
model.add(layers.MaxPooling2D((2,2)))
model.add(layers.Conv2D(20, (5, 5), activation='relu'))
model.add(layers.MaxPooling2D((2,2)))
model.add(layers.Flatten())
model.add(layers.Dense(100, activation='relu'))
model.add(layers.Dense(10, activation='softmax'))

from keras import optimizers

# Compile model
model.compile(optimizers.RMSprop(lr=0.0001),
             loss='categorical_crossentropy',
             metrics=['accuracy'])

# Train model
history = model.fit(x_train_vec, y_train_vec,
                   batch_size=128, epochs=50,
                   validation_data=(x_valid_vec, y_valid_vec))

# Print loss and accuracy
loss_and_acc = model.evaluate(x_test_vec, y_test_vec)
print('loss = ' + str(loss_and_acc[0]))
